% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/information_theory.R
\name{cross_entropy}
\alias{cross_entropy}
\title{Cross entropy between texts}
\usage{
cross_entropy(text_p, text_q, level = c("word", "letter"))
}
\arguments{
\item{text_p}{local text, or local data frame}

\item{text_q}{global text, or local data frame}

\item{level}{"word" or "letter"}
}
\value{
Cross entropy in bits
}
\description{
It calculates the cross entropy (that is, H(P, Q), where P is the local distribution (text_p) and Q is a global distribution
}
\examples{
# Example of comparison between two texts on word level (Shannon, 1939)
text1 <- "Dear Dr. Bush, Off and on I have been working on an analysis of some of the fundamental properties of general systems for the transmission of intelligence, including telephony, radio, television, telegraphy, etc."
text2 <- "Of course, my main project is still the machine for performing symbolic mathematical operations; although I have made some progress in various outskirts of the problem I am still pretty much in the woods, so far as actual results are concerned and so can't tell you much about it. I have a set of circuits drawn up which actually will perform symbolic differentiation and integration on most functions, but the method is not quite general or natural enough to be perfectly satisfactory. Some of the general philosophy underlying the machine seems to evade me completely..."
cross_entropy(text1, text2, level = "word")

# Example of comparison between two datasets (selected_piantadosi and heart_of_darkness, available in langstats, remember to load them with data(dataset_name))
data(selected_piantadosi)
data(heart_of_darkness)


cross_entropy(selected_piantadosi, heart_of_darkness, level = "word")

}
